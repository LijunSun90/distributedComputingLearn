######################################################################
#####
## Cluster Setup to set up a multi-node Spark installation on Centos 7 / Ubuntu
#####
##
## !!! This document is based on the guide for Hadoop cluster setup. !!!
##
## Step-01
## Step-02
## Step-03
## Step-04
#####
######################################################################



##——————————————————————Step—01—————————————————————————##
####################
[myname@master]$ cd ~/softwares
[myname@master]$ tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz
[myname@master]$ mv spark-2.1.1-bin-hadoop2.7 spark
[myname@master]$ cd spark
[myname@master]$ mkdir logs
[myname@master]$ cd conf/
[myname@master]$ cp spark-env.sh.template spark-env.sh
[myname@master]$ vi spark-env.sh
#
##
###
# Options read in YARN client mode
# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
export HADOOP_CONF_DIR=/home/myname/softwares/hadoop/etc/hadoop
# - SPARK_EXECUTOR_INSTANCES, Number of executors to start (Default: 2)
# - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).
# - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)
# - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)

# Options for the daemons used in the standalone deploy mode
# - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname
export SPARK_MASTER_HOST=dc001
# - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
# - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
# - SPARK_WORKER_CORES, to set the number of cores to use on this machine
# - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
# - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
# - SPARK_WORKER_INSTANCES, to set the number of worker processes per node
# - SPARK_WORKER_DIR, to set the working directory of worker processes
export SPARK_WORKER_DIR=/home/myname/data/spark/work
# - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
# - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).
# - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
# - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. "-Dx=y")
# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers

# Generic options for the daemons used in the standalone deploy mode
# - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
# - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
export SPARK_LOG_DIR=/home/myname/softwares/spark/logs
# - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
# - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
# - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
###
##
#
[myname@master]$ cp slaves.template slaves
[myname@master]$ vi slaves
#
##
###
# A Spark Worker will be started on each of the machines listed below.
master
slave1
slave2
slave3
###
##
#
##——————————————————————Step—01—————————————————————————##


##——————————————————————Step—02—————————————————————————##
## Replicate the spark files
[myname@master]$ scp -r spark myname@slave1:/home/myname/softwares
[myname@master]$ scp -r spark myname@slave2:/home/myname/softwares
[myname@master]$ scp -r spark myname@slave3:/home/myname/softwares
##——————————————————————Step—02—————————————————————————##


##——————————————————————Step—03—————————————————————————##
#
## Verify
#
[myname@master]$ ~/softwares/spark/sbin/start-all.sh
[myname@master]$ jps
#
[myname@slave1]$ jps
#
[myname@slave2]$ jps
#
[myname@slave3]$ jps
#
[myname@master]$ ~/softwares/spark/sbin/stop-all.sh
##——————————————————————Step—03—————————————————————————##


##——————————————————————Step—04—————————————————————————##
#
[myname@master]$ sudo sh -c "echo 'export SPARK_HOME=/home/duanqq/softwares/spark' >> /etc/profile"
[myname@master]$ sudo sh -c "echo 'export PATH=${PATH}:${SPARK_HOME}/bin:/${SPARK_HOME}/sbin' >> /etc/profile"
[myname@master]$ source /etc/profile
#
[myname@slave1]$ sudo sh -c "echo 'export SPARK_HOME=/home/duanqq/softwares/spark' >> /etc/profile"
[myname@slave1]$ sudo sh -c "echo 'export PATH=${PATH}:${SPARK_HOME}/bin:/${SPARK_HOME}/sbin' >> /etc/profile"
[myname@slave1]$ source /etc/profile
#
[myname@slave2]$ sudo sh -c "echo 'export SPARK_HOME=/home/duanqq/softwares/spark' >> /etc/profile"
[myname@slave2]$ sudo sh -c "echo 'export PATH=${PATH}:${SPARK_HOME}/bin:/${SPARK_HOME}/sbin' >> /etc/profile"
[myname@slave2]$ source /etc/profile
#
[myname@slave3]$ sudo sh -c "echo 'export SPARK_HOME=/home/duanqq/softwares/spark' >> /etc/profile"
[myname@slave3]$ sudo sh -c "echo 'export PATH=${PATH}:${SPARK_HOME}/bin:/${SPARK_HOME}/sbin' >> /etc/profile"
[myname@slave3]$ source /etc/profile
##——————————————————————Step—04—————————————————————————##

